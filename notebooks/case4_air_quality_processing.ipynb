{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font size=\"6\"><b> CASE - air quality data of European monitoring stations (AirBase)</b></font></p>\n",
    "\n",
    "> *© 2021, Joris Van den Bossche and Stijn Van Hoey  (<mailto:jorisvandenbossche@gmail.com>, <mailto:stijnvanhoey@gmail.com>). Licensed under [CC BY 4.0 Creative Commons](http://creativecommons.org/licenses/by/4.0/)*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AirBase** is the European air quality database maintained by the European Environment Agency (EEA). It contains air quality monitoring data and information submitted by participating countries throughout Europe. The [air quality database](https://www.eea.europa.eu/data-and-maps/data/aqereporting-8/air-quality-zone-geometries) consists of a multi-annual time series of air quality measurement data and statistics for a number of air pollutants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the data files that are available from AirBase were included in the data folder: the **hourly concentrations of nitrogen dioxide (NO2)** for 4 different measurement stations:\n",
    "\n",
    "- FR04037 (PARIS 13eme): urban background site at Square de Choisy\n",
    "- FR04012 (Paris, Place Victor Basch): urban traffic site at Rue d'Alesia\n",
    "- BETR802: urban traffic site in Antwerp, Belgium\n",
    "- BETN029: rural background site in Houtem, Belgium\n",
    "\n",
    "See http://www.eea.europa.eu/themes/air/interactive/no2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing a single file\n",
    "\n",
    "We will start with processing one of the downloaded files (`BETR8010000800100hour.1-1-1990.31-12-2012`). Looking at the data, you will see it does not look like a nice csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/BETR8010000800100hour.1-1-1990.31-12-2012\") as f:\n",
    "    print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will need to do some manual processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just reading the tab-delimited data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/BETR8010000800100hour.1-1-1990.31-12-2012\", sep='\\t')#, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above data is clearly not ready to be used! Each row contains the 24 measurements for each hour of the day, and also contains a flag (0/1) indicating the quality of the data. Furthermore, there is no header row with column names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE 1</b>: <br><br> Clean up this dataframe by using more options of `pd.read_csv` (see its [docstring](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html))\n",
    "\n",
    " <ul>\n",
    "  <li>specify the correct delimiter</li>\n",
    "  <li>specify that the values of -999 and -9999 should be regarded as NaN</li>\n",
    "  <li>specify our own column names (for how the column names are made up, see <a href=\"http://stackoverflow.com/questions/6356041/python-intertwining-two-lists\">http://stackoverflow.com/questions/6356041/python-intertwining-two-lists</a>)\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names: list consisting of 'date' and then intertwined the hour of the day and 'flag'\n",
    "hours = [\"{:02d}\".format(i) for i in range(24)]\n",
    "column_names = ['date'] + [item for pair in zip(hours, ['flag' + str(i) for i in range(24)]) for item in pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbtutor-solution"
    ]
   },
   "outputs": [],
   "source": [
    "# %load _solutions/case4_air_quality_processing1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbtutor-solution"
    ]
   },
   "outputs": [],
   "source": [
    "# %load _solutions/case4_air_quality_processing2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of this tutorial, we will disregard the 'flag' columns (indicating the quality of the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**EXERCISE 2**:\n",
    "\n",
    "Drop all 'flag' columns ('flag1', 'flag2', ...)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_columns = [col for col in data.columns if 'flag' in col]\n",
    "# we can now use this list to drop these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbtutor-solution"
    ]
   },
   "outputs": [],
   "source": [
    "# %load _solutions/case4_air_quality_processing3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to reshape it: our goal is to have the different hours as row indices, merged with the date into a datetime-index. Here we have a wide and long dataframe, and want to make this a long, narrow timeseries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>REMEMBER</b>: \n",
    "\n",
    "\n",
    "Recap: reshaping your data with [`stack` / `melt` and `unstack` / `pivot`](./pandas_08_reshaping_data.ipynb)</li>\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../img/pandas/schema-stack.svg\" width=70%>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE 3</b>:\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Reshape the dataframe to a timeseries. \n",
    "The end result should look like:<br><br>\n",
    "\n",
    "\n",
    "<div class='center'>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>BETR801</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>1990-01-02 09:00:00</th>\n",
    "      <td>48.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1990-01-02 12:00:00</th>\n",
    "      <td>48.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1990-01-02 13:00:00</th>\n",
    "      <td>50.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1990-01-02 14:00:00</th>\n",
    "      <td>55.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>...</th>\n",
    "      <td>...</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2012-12-31 20:00:00</th>\n",
    "      <td>16.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2012-12-31 21:00:00</th>\n",
    "      <td>14.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2012-12-31 22:00:00</th>\n",
    "      <td>16.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2012-12-31 23:00:00</th>\n",
    "      <td>15.0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "<p style=\"text-align:center\">170794 rows × 1 columns</p>\n",
    "</div>\n",
    "\n",
    " <ul>\n",
    "  <li>Reshape the dataframe so that each row consists of one observation for one date + hour combination</li>\n",
    "  <li>When you have the date and hour values as two columns, combine these columns into a datetime (tip: string columns can be summed to concatenate the strings) and remove the original columns</li>\n",
    "  <li>Set the new datetime values as the index, and remove the original columns with date and hour values</li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "\n",
    "**NOTE**: This is an advanced exercise. Do not spend too much time on it and don't hesitate to look at the solutions. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping using `melt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbtutor-solution"
    ]
   },
   "outputs": [],
   "source": [
    "# %load _solutions/case4_air_quality_processing4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping using `stack`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbtutor-solution"
    ]
   },
   "outputs": [],
   "source": [
    "# %load _solutions/case4_air_quality_processing5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbtutor-solution"
    ]
   },
   "outputs": [],
   "source": [
    "# %load _solutions/case4_air_quality_processing6.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine date and hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbtutor-solution"
    ]
   },
   "outputs": [],
   "source": [
    "# %load _solutions/case4_air_quality_processing7.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbtutor-solution"
    ]
   },
   "outputs": [],
   "source": [
    "# %load _solutions/case4_air_quality_processing8.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbtutor-solution"
    ]
   },
   "outputs": [],
   "source": [
    "# %load _solutions/case4_air_quality_processing9.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stacked.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final data is now a time series. In pandas, this means that the index is a `DatetimeIndex`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stacked.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stacked.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing a collection of files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have seen the code steps to process one of the files. We have however multiple files for the different stations with the same structure. Therefore, to not have to repeat the actual code, let's make a function from the steps we have seen above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE 4</b>:\n",
    "\n",
    " <ul>\n",
    "    <li>Write a function <code>read_airbase_file(filename, station)</code>, using the above steps the read in and process the data, and that returns a processed timeseries.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_airbase_file(filename, station):\n",
    "    \"\"\"\n",
    "    Read hourly AirBase data files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : string\n",
    "        Path to the data file.\n",
    "    station : string\n",
    "        Name of the station.\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Processed dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbtutor-solution"
    ]
   },
   "outputs": [],
   "source": [
    "# %load _solutions/case4_air_quality_processing10.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function on the data file from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/BETR8010000800100hour.1-1-1990.31-12-2012\"\n",
    "station = os.path.split(filename)[-1][:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_airbase_file(filename, station)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to use this function to read in all the different data files from AirBase, and combine them into one Dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**EXERCISE 5**:\n",
    "\n",
    "Use the [pathlib module](https://docs.python.org/3/library/pathlib.html) `Path` class in combination with the `glob` method to list all 4 AirBase data files that are included in the 'data' directory, and call the result `data_files`.\n",
    "\n",
    "<details><summary>Hints</summary>\n",
    "\n",
    "- The pathlib module provides a object oriented way to handle file paths. First, create a `Path` object of the data folder, `pathlib.Path(\"./data\")`. Next, apply the `glob` function to extract all the files containing `*0008001*` (use wildcard * to say \"any characters\"). The output is a Python generator, which you can collect as a `list()`.\n",
    "\n",
    "</details>    \n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbtutor-solution"
    ]
   },
   "outputs": [],
   "source": [
    "# %load _solutions/case4_air_quality_processing11.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**EXERCISE 6**:\n",
    "\n",
    "* Loop over the data files, read and process the file using our defined function, and append the dataframe to a list.\n",
    "* Combine the the different DataFrames in the list into a single DataFrame where the different columns are the different stations. Call the result `combined_data`.\n",
    "\n",
    "<details><summary>Hints</summary>\n",
    "\n",
    "- The `data_files` list contains `Path` objects (from the pathlib module). To get the actual file name as a string, use the `.name` attribute.\n",
    "- The station name is always first 7 characters of the file name.\n",
    "\n",
    "</details>    \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbtutor-solution"
    ]
   },
   "outputs": [],
   "source": [
    "# %load _solutions/case4_air_quality_processing12.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbtutor-solution"
    ]
   },
   "outputs": [],
   "source": [
    "# %load _solutions/case4_air_quality_processing13.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we don't want to have to repeat this each time we use the data. Therefore, let's save the processed data to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first give the index a descriptive name\n",
    "combined_data.index.name = 'datetime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.to_csv(\"airbase_data_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
